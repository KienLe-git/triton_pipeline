{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['inception_graphdef', 'densenet_onnx']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_model = [i.split('/')[-1] for i in glob.glob('model_repository/[!.]*')]\n",
    "ls_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3361, 2521), 'RGB')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im = Image.open('data/mug.jpg')\n",
    "im.size, im.mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inception_graphdef\n",
      "Request 1, batch size 1\n",
      "    0.826934 (505) = COFFEE MUG\n",
      "    0.124097 (969) = CUP\n",
      "    0.002272 (900) = WATER JUG\n",
      "PASS\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "densenet_onnx\n",
      "Request 1, batch size 1\n",
      "    13.915169 (504) = COFFEE MUG\n",
      "    12.018229 (968) = CUP\n",
      "    9.839120 (967) = ESPRESSO\n",
      "PASS\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in ls_model:\n",
    "    print(model)\n",
    "    ! python image_client.py -m {model} -c 3 -s INCEPTION -i http -u localhost:8000 data/mug.jpg\n",
    "    print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  5867  100  5867    0     0   209k      0 --:--:-- --:--:-- --:--:--  954k\n",
      "# HELP nv_inference_request_success Number of successful inference requests, all batch sizes\n",
      "# TYPE nv_inference_request_success counter\n",
      "nv_inference_request_success{model=\"inception_graphdef\",version=\"1\"} 1.000000\n",
      "nv_inference_request_success{model=\"densenet_onnx\",version=\"1\"} 1.000000\n",
      "# HELP nv_inference_request_failure Number of failed inference requests, all batch sizes\n",
      "# TYPE nv_inference_request_failure counter\n",
      "nv_inference_request_failure{model=\"inception_graphdef\",version=\"1\"} 0.000000\n",
      "nv_inference_request_failure{model=\"densenet_onnx\",version=\"1\"} 0.000000\n",
      "# HELP nv_inference_count Number of inferences performed (does not include cached requests)\n",
      "# TYPE nv_inference_count counter\n",
      "nv_inference_count{model=\"inception_graphdef\",version=\"1\"} 1.000000\n",
      "nv_inference_count{model=\"densenet_onnx\",version=\"1\"} 1.000000\n",
      "# HELP nv_inference_exec_count Number of model executions performed (does not include cached requests)\n",
      "# TYPE nv_inference_exec_count counter\n",
      "nv_inference_exec_count{model=\"inception_graphdef\",version=\"1\"} 1.000000\n",
      "nv_inference_exec_count{model=\"densenet_onnx\",version=\"1\"} 1.000000\n",
      "# HELP nv_inference_request_duration_us Cumulative inference request duration in microseconds (includes cached requests)\n",
      "# TYPE nv_inference_request_duration_us counter\n",
      "nv_inference_request_duration_us{model=\"inception_graphdef\",version=\"1\"} 2118475.000000\n",
      "nv_inference_request_duration_us{model=\"densenet_onnx\",version=\"1\"} 111575.000000\n",
      "# HELP nv_inference_queue_duration_us Cumulative inference queuing duration in microseconds (includes cached requests)\n",
      "# TYPE nv_inference_queue_duration_us counter\n",
      "nv_inference_queue_duration_us{model=\"inception_graphdef\",version=\"1\"} 597.000000\n",
      "nv_inference_queue_duration_us{model=\"densenet_onnx\",version=\"1\"} 600.000000\n",
      "# HELP nv_inference_compute_input_duration_us Cumulative compute input duration in microseconds (does not include cached requests)\n",
      "# TYPE nv_inference_compute_input_duration_us counter\n",
      "nv_inference_compute_input_duration_us{model=\"inception_graphdef\",version=\"1\"} 2286.000000\n",
      "nv_inference_compute_input_duration_us{model=\"densenet_onnx\",version=\"1\"} 1307.000000\n",
      "# HELP nv_inference_compute_infer_duration_us Cumulative compute inference duration in microseconds (does not include cached requests)\n",
      "# TYPE nv_inference_compute_infer_duration_us counter\n",
      "nv_inference_compute_infer_duration_us{model=\"inception_graphdef\",version=\"1\"} 2114516.000000\n",
      "nv_inference_compute_infer_duration_us{model=\"densenet_onnx\",version=\"1\"} 109187.000000\n",
      "# HELP nv_inference_compute_output_duration_us Cumulative inference compute output duration in microseconds (does not include cached requests)\n",
      "# TYPE nv_inference_compute_output_duration_us counter\n",
      "nv_inference_compute_output_duration_us{model=\"inception_graphdef\",version=\"1\"} 368.000000\n",
      "nv_inference_compute_output_duration_us{model=\"densenet_onnx\",version=\"1\"} 287.000000\n",
      "# HELP nv_cache_num_entries Number of responses stored in response cache\n",
      "# TYPE nv_cache_num_entries gauge\n",
      "# HELP nv_cache_num_lookups Number of cache lookups in response cache\n",
      "# TYPE nv_cache_num_lookups gauge\n",
      "# HELP nv_cache_num_hits Number of cache hits in response cache\n",
      "# TYPE nv_cache_num_hits gauge\n",
      "# HELP nv_cache_num_misses Number of cache misses in response cache\n",
      "# TYPE nv_cache_num_misses gauge\n",
      "# HELP nv_cache_num_evictions Number of cache evictions in response cache\n",
      "# TYPE nv_cache_num_evictions gauge\n",
      "# HELP nv_cache_lookup_duration Total cache lookup duration (hit and miss), in microseconds\n",
      "# TYPE nv_cache_lookup_duration gauge\n",
      "# HELP nv_cache_insertion_duration Total cache insertion duration, in microseconds\n",
      "# TYPE nv_cache_insertion_duration gauge\n",
      "# HELP nv_cache_util Cache utilization [0.0 - 1.0]\n",
      "# TYPE nv_cache_util gauge\n",
      "# HELP nv_cache_num_hits_per_model Number of cache hits per model\n",
      "# TYPE nv_cache_num_hits_per_model counter\n",
      "nv_cache_num_hits_per_model{model=\"inception_graphdef\",version=\"1\"} 0.000000\n",
      "nv_cache_num_hits_per_model{model=\"densenet_onnx\",version=\"1\"} 0.000000\n",
      "# HELP nv_cache_hit_lookup_duration_per_model Total cache hit lookup duration per model, in microseconds\n",
      "# TYPE nv_cache_hit_lookup_duration_per_model counter\n",
      "nv_cache_hit_lookup_duration_per_model{model=\"inception_graphdef\",version=\"1\"} 0.000000\n",
      "nv_cache_hit_lookup_duration_per_model{model=\"densenet_onnx\",version=\"1\"} 0.000000\n",
      "# HELP nv_cache_num_misses_per_model Number of cache misses per model\n",
      "# TYPE nv_cache_num_misses_per_model counter\n",
      "nv_cache_num_misses_per_model{model=\"inception_graphdef\",version=\"1\"} 0.000000\n",
      "nv_cache_num_misses_per_model{model=\"densenet_onnx\",version=\"1\"} 0.000000\n",
      "# HELP nv_cache_miss_lookup_duration_per_model Total cache miss lookup duration per model, in microseconds\n",
      "# TYPE nv_cache_miss_lookup_duration_per_model counter\n",
      "nv_cache_miss_lookup_duration_per_model{model=\"inception_graphdef\",version=\"1\"} 0.000000\n",
      "nv_cache_miss_lookup_duration_per_model{model=\"densenet_onnx\",version=\"1\"} 0.000000\n",
      "# HELP nv_cache_miss_insertion_duration_per_model Total cache miss insertion duration per model, in microseconds\n",
      "# TYPE nv_cache_miss_insertion_duration_per_model counter\n",
      "nv_cache_miss_insertion_duration_per_model{model=\"inception_graphdef\",version=\"1\"} 0.000000\n",
      "nv_cache_miss_insertion_duration_per_model{model=\"densenet_onnx\",version=\"1\"} 0.000000\n",
      "# HELP nv_gpu_utilization GPU utilization rate [0.0 - 1.0)\n",
      "# TYPE nv_gpu_utilization gauge\n",
      "# HELP nv_gpu_memory_total_bytes GPU total memory, in bytes\n",
      "# TYPE nv_gpu_memory_total_bytes gauge\n",
      "# HELP nv_gpu_memory_used_bytes GPU used memory, in bytes\n",
      "# TYPE nv_gpu_memory_used_bytes gauge\n",
      "# HELP nv_gpu_power_usage GPU power usage in watts\n",
      "# TYPE nv_gpu_power_usage gauge\n",
      "# HELP nv_gpu_power_limit GPU power management limit in watts\n",
      "# TYPE nv_gpu_power_limit gauge\n",
      "# HELP nv_energy_consumption GPU energy consumption in joules since the Triton Server started\n",
      "# TYPE nv_energy_consumption counter\n"
     ]
    }
   ],
   "source": [
    "! curl localhost:8002/metrics -o health.txt && cat health.txt\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8af044a55135b082f754ddacc897481f600d1526dc10d902184e16bc01f940b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
